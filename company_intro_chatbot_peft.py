import streamlit as st
import tiktoken
from loguru import logger
import torch

from langchain_core.messages import ChatMessage

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.document_loaders import UnstructuredPowerPointLoader

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# PEFT ê´€ë ¨ import
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel
from langchain_community.llms import HuggingFacePipeline
from sentence_transformers import SentenceTransformer

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
LLM_LORA_PATH = "./llm_lora_model"
EMBEDDING_PATH = "./embedding_finetuned_model"
BASE_LLM_MODEL = "beomi/Llama-3-Open-Ko-8B"

def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)

def get_text(docs):

    doc_list = []

    for doc in docs:
        file_name = doc.name  # doc ê°ì²´ì˜ ì´ë¦„ì„ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
        with open(file_name, "wb") as file:  # íŒŒì¼ì„ doc.nameìœ¼ë¡œ ì €ì¥
            file.write(doc.getvalue())
            logger.info(f"Uploaded {file_name}")
        if '.pdf' in doc.name:
            loader = PyPDFLoader(file_name)
            documents = loader.load_and_split()
        elif '.docx' in doc.name:
            loader = Docx2txtLoader(file_name)
            documents = loader.load_and_split()
        elif '.pptx' in doc.name:
            loader = UnstructuredPowerPointLoader(file_name)
            documents = loader.load_and_split()

        doc_list.extend(documents)
    return doc_list

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100,
        length_function=tiktoken_len
    )
    chunks = text_splitter.split_documents(text)
    return chunks

class SentenceTransformerEmbeddings:
    """SentenceTransformerë¥¼ LangChainì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë˜í¼"""
    def __init__(self, model_path):
        self.model = SentenceTransformer(model_path)

    def embed_documents(self, texts):
        return self.model.encode(texts).tolist()

    def embed_query(self, text):
        return self.model.encode([text])[0].tolist()

def get_vectorstore(text_chunks):
    # íŒŒì¸íŠœë‹ëœ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    embeddings = SentenceTransformerEmbeddings(EMBEDDING_PATH)
    vectordb = FAISS.from_documents(text_chunks, embeddings)

    return vectordb

@st.cache_resource
def load_peft_model():
    """íŒŒì¸íŠœë‹ëœ LoRA ëª¨ë¸ ë¡œë“œ (ìºì‹±)"""
    st.info("íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(LLM_LORA_PATH)

    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_LLM_MODEL,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    model = PeftModel.from_pretrained(base_model, LLM_LORA_PATH)

    # Pipeline ìƒì„±
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1
    )

    # LangChain LLMìœ¼ë¡œ ë˜í•‘
    llm = HuggingFacePipeline(pipeline=pipe)

    st.success("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    return llm

def extract_text_from_chunk(chunk):
    """ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if isinstance(chunk, str):
        return chunk
    elif isinstance(chunk, dict):
        content = chunk.get('content', '') or chunk.get('text', '')
        return content if content else ''
    elif hasattr(chunk, 'content'):
        return chunk.content if chunk.content else ''
    else:
        return ''

def main():

    global retriever

    st.set_page_config(
    page_title="ì±—ë´‡",
    page_icon="")

    st.title(":blue[ì±—ë´‡]")
    st.caption("ğŸ¤– PEFT íŒŒì¸íŠœë‹ ì ìš© ë²„ì „")

    if "messages" not in st.session_state:
       st.session_state["messages"] = []

   #ì±„íŒ… ëŒ€í™”ê¸°ë¡ì„ ì ê²€
    if "store" not in st.session_state:
       st.session_state["store"] =dict()


    def print_history():
        for msg in st.session_state.messages:
            st.chat_message(msg.role).write(msg.content)

    def add_history(role, content):
        st.session_state.messages.append(ChatMessage(role=role, content=content))

    if "processComplete" not in st.session_state:
        st.session_state.processComplete = None

    if "retriever" not in st.session_state:
        st.session_state.retriever = None

    with st.sidebar:
        st.header("ìë£Œ ì—…ë¡œë“œ")
        uploaded_files =  st.file_uploader("PDF, DOCX, PPTX íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
                                          type=['pdf','docx','pptx'],
                                          accept_multiple_files=True)
        process = st.button("ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°")

    if process:

        files_text = get_text(uploaded_files)
        text_chunks = get_text_chunks(files_text)
        vectorstore = get_vectorstore(text_chunks)
        retriever = vectorstore.as_retriever(search_type = 'mmr', vervose = True)
        st.session_state['retriever'] =retriever

        st.session_state.processComplete = True

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [{"role": "assistant",
                                        "content": "ì•ˆë…•í•˜ì„¸ìš”! Future Systems íšŒì‚¬ì†Œê°œ ì±—ë´‡ì…ë‹ˆë‹¤. íšŒì‚¬ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"}]

    def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
        return "\n\n".join(doc.page_content for doc in docs)

    RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ Future Systems íšŒì‚¬ ì†Œê°œ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                             ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ íšŒì‚¬ì— ëŒ€í•œ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                             ë‹µë³€ì€ êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ë˜, ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.

                             Question: {question}
                             Context: {context}
                             Answer:"""

    print_history()

    if user_input := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”..."):
        #ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
        add_history("user", user_input)
        st.chat_message("user").write(f"{user_input}")
        with st.chat_message("assistant"):

            # íŒŒì¸íŠœë‹ëœ ë¡œì»¬ LLM ì‚¬ìš©
            llm = load_peft_model()
            chat_container = st.empty()

            if  st.session_state.processComplete==True:
                prompt1 = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

                retriever = st.session_state['retriever']
               # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
                rag_chain = (
                   {
                       "context": retriever | format_docs,
                       "question": RunnablePassthrough(),
                   }
                   | prompt1
                   | llm
                )

                # ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì¼ë°˜ í˜¸ì¶œ
                answer = rag_chain.invoke(user_input)
                response_text = extract_text_from_chunk(answer)
                chat_container.markdown(response_text)
                add_history("ai", response_text)

            else:
                prompt2 = ChatPromptTemplate.from_template(
                    "ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ë‹¨íˆ ë‹µë³€í•´ì£¼ì„¸ìš”. ë§Œì•½ íšŒì‚¬ ê´€ë ¨ ì§ˆë¬¸ì´ë¼ë©´, ë¨¼ì € íšŒì‚¬ ì†Œê°œ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ë„ë¡ ì•ˆë‚´í•´ì£¼ì„¸ìš”:\n{input}"
                )

                # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
                chain = prompt2 | llm

                answer = chain.invoke(user_input)
                response_text = extract_text_from_chunk(answer)
                chat_container.markdown(response_text)
                add_history("ai", response_text)

if __name__ == '__main__':
    main()
