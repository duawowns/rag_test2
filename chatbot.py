import streamlit as st
import tiktoken
from loguru import logger
import pandas as pd

from langchain_core.messages import ChatMessage
from langchain.schema import Document

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langserve import RemoteRunnable

# ì„¤ì •
CSV_DATA_PATH = "company_data.csv"
PDF_DATA_PATH = "futuresystems_company_brochure.pdf"
DEFAULT_LLM_URL = "https://dioramic-corrin-undetractively.ngrok-free.dev/llm/"

def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)

def load_csv_data(csv_path):
    """CSV ë°ì´í„°ë¥¼ Document í˜•ì‹ìœ¼ë¡œ ë¡œë“œ"""
    df = pd.read_csv(csv_path)
    documents = []

    for idx, row in df.iterrows():
        content = f"""ì´ë¦„: {row['ì´ë¦„']}
ì§ê¸‰: {row['ì§ê¸‰']}
ë¶€ì„œ: {row['ë¶€ì„œ']}
ì „í™”ë²ˆí˜¸: {row['ì „í™”ë²ˆí˜¸']}
ì´ë©”ì¼: {row['ì´ë©”ì¼']}
ì…ì‚¬ì¼: {row['ì…ì‚¬ì¼']}
ë‹´ë‹¹ì—…ë¬´: {row['ë‹´ë‹¹ì—…ë¬´']}"""

        doc = Document(
            page_content=content,
            metadata={
                "source": "company_data.csv",
                "name": row['ì´ë¦„'],
                "position": row['ì§ê¸‰'],
                "phone": row['ì „í™”ë²ˆí˜¸']
            }
        )
        documents.append(doc)

    return documents

def load_pdf_data(pdf_path):
    """PDF ë°ì´í„° ë¡œë“œ"""
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    return documents

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100,
        length_function=tiktoken_len
    )
    chunks = text_splitter.split_documents(text)
    return chunks

def get_vectorstore(text_chunks):
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    vectordb = FAISS.from_documents(text_chunks, embeddings)
    return vectordb

def extract_text_from_chunk(chunk):
    """ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if isinstance(chunk, str):
        return chunk
    elif isinstance(chunk, dict):
        content = chunk.get('content', '') or chunk.get('text', '')
        return content if content else ''
    elif hasattr(chunk, 'content'):
        return chunk.content if chunk.content else ''
    else:
        return ''

@st.cache_resource
def initialize_rag_system():
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìë™)"""
    with st.spinner("ë°ì´í„° ë¡œë”© ì¤‘..."):
        all_docs = []

        # CSV ë¡œë“œ
        try:
            csv_docs = load_csv_data(CSV_DATA_PATH)
            all_docs.extend(csv_docs)
            st.success(f"âœ… ì§ì› ë°ì´í„° ë¡œë“œ: {len(csv_docs)}ëª…")
        except Exception as e:
            st.warning(f"CSV ë¡œë“œ ì‹¤íŒ¨: {e}")

        # PDF ë¡œë“œ
        try:
            pdf_docs = load_pdf_data(PDF_DATA_PATH)
            pdf_chunks = get_text_chunks(pdf_docs)
            all_docs.extend(pdf_chunks)
            st.success(f"âœ… íšŒì‚¬ ì†Œê°œ ìë£Œ ë¡œë“œ: {len(pdf_chunks)}ê°œ ì²­í¬")
        except Exception as e:
            st.warning(f"PDF ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        if all_docs:
            vectorstore = get_vectorstore(all_docs)
            retriever = vectorstore.as_retriever(
                search_type='mmr',
                search_kwargs={'k': 5, 'fetch_k': 10}
            )
            st.success(f"âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ: ì´ {len(all_docs)}ê°œ ë¬¸ì„œ")
            return retriever
        else:
            st.error("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

def main():
    st.set_page_config(
        page_title="ì±—ë´‡",
        page_icon=""
    )

    st.title(":blue[ì±—ë´‡]")
    st.caption("ğŸš€ RAG í•˜ì´ë¸Œë¦¬ë“œ ì±—ë´‡ - ìë™ ë¡œë“œ")

    # ë©”ì‹œì§€ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ì„¤ì •")

        llm_url = st.text_input(
            "LLM ì„œë²„ URL",
            value=DEFAULT_LLM_URL,
            help="ngrok URLì´ ë³€ê²½ë˜ë©´ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”"
        )

        st.divider()
        st.info("""
        **ìë™ ë¡œë“œë¨:**
        - ì§ì› ë°ì´í„° (CSV)
        - íšŒì‚¬ ì†Œê°œ (PDF)

        ë°”ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”!
        """)

    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìë™, ìºì‹±)
    retriever = initialize_rag_system()

    if "messages" not in st.session_state or len(st.session_state["messages"]) == 0:
        st.session_state["messages"] = [
            ChatMessage(role="assistant", content="ì•ˆë…•í•˜ì„¸ìš”! íšŒì‚¬ ë° ì§ì› ì •ë³´ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
        ]

    # ëŒ€í™” ê¸°ë¡ ì¶œë ¥
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

    RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ Future Systems íšŒì‚¬ ì†Œê°œ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ íšŒì‚¬ ë° ì§ì› ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
íŠ¹íˆ ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ë¶€ì„œ ë“±ì˜ ì •ë³´ëŠ” ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì œê³µí•˜ì„¸ìš”.

Question: {question}
Context: {context}
Answer:"""

    # ì‚¬ìš©ì ì…ë ¥
    if user_input := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append(ChatMessage(role="user", content=user_input))
        st.chat_message("user").write(user_input)

        # AI ì‘ë‹µ
        with st.chat_message("assistant"):
            chat_container = st.empty()

            if retriever:
                try:
                    # LLM ì—°ê²°
                    llm = RemoteRunnable(llm_url)

                    # í”„ë¡¬í”„íŠ¸ ìƒì„±
                    prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

                    # ë¬¸ì„œ í¬ë§·íŒ…
                    def format_docs(docs):
                        return "\n\n".join(doc.page_content for doc in docs)

                    # RAG ì²´ì¸
                    rag_chain = (
                        {
                            "context": retriever | format_docs,
                            "question": RunnablePassthrough(),
                        }
                        | prompt
                        | llm
                    )

                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                    answer = rag_chain.stream(user_input)
                    chunks = []
                    for chunk in answer:
                        chunk_text = extract_text_from_chunk(chunk)
                        if chunk_text:
                            chunks.append(chunk_text)
                            chat_container.markdown("".join(chunks))

                    # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                    st.session_state.messages.append(
                        ChatMessage(role="assistant", content="".join(chunks))
                    )

                except Exception as e:
                    error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    chat_container.error(error_msg)
                    st.session_state.messages.append(
                        ChatMessage(role="assistant", content=error_msg)
                    )
            else:
                error_msg = "RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                chat_container.error(error_msg)
                st.session_state.messages.append(
                    ChatMessage(role="assistant", content=error_msg)
                )

if __name__ == '__main__':
    main()
