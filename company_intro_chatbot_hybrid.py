import streamlit as st
import tiktoken
from loguru import logger
import torch
import pandas as pd

from langchain_core.messages import ChatMessage
from langchain.schema import Document

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.document_loaders import UnstructuredPowerPointLoader

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# PEFT ê´€ë ¨ import
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel
from langchain_community.llms import HuggingFacePipeline
from sentence_transformers import SentenceTransformer

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
LLM_LORA_PATH = "./llm_lora_model"
EMBEDDING_PATH = "./embedding_finetuned_model"
BASE_LLM_MODEL = "beomi/Llama-3-Open-Ko-8B"
CSV_DATA_PATH = "company_data.csv"

def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)

def load_csv_data(csv_path):
    """CSV ë°ì´í„°ë¥¼ Document í˜•ì‹ìœ¼ë¡œ ë¡œë“œ"""
    df = pd.read_csv(csv_path)
    documents = []

    for idx, row in df.iterrows():
        # ê° ì§ì› ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ë³€í™˜
        content = f"""ì´ë¦„: {row['ì´ë¦„']}
ì§ê¸‰: {row['ì§ê¸‰']}
ë¶€ì„œ: {row['ë¶€ì„œ']}
ì „í™”ë²ˆí˜¸: {row['ì „í™”ë²ˆí˜¸']}
ì´ë©”ì¼: {row['ì´ë©”ì¼']}
ì…ì‚¬ì¼: {row['ì…ì‚¬ì¼']}
ë‹´ë‹¹ì—…ë¬´: {row['ë‹´ë‹¹ì—…ë¬´']}"""

        doc = Document(
            page_content=content,
            metadata={
                "source": "company_data.csv",
                "name": row['ì´ë¦„'],
                "position": row['ì§ê¸‰'],
                "phone": row['ì „í™”ë²ˆí˜¸']
            }
        )
        documents.append(doc)

    return documents

def get_text(docs):

    doc_list = []

    for doc in docs:
        file_name = doc.name  # doc ê°ì²´ì˜ ì´ë¦„ì„ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
        with open(file_name, "wb") as file:  # íŒŒì¼ì„ doc.nameìœ¼ë¡œ ì €ì¥
            file.write(doc.getvalue())
            logger.info(f"Uploaded {file_name}")
        if '.pdf' in doc.name:
            loader = PyPDFLoader(file_name)
            documents = loader.load_and_split()
        elif '.docx' in doc.name:
            loader = Docx2txtLoader(file_name)
            documents = loader.load_and_split()
        elif '.pptx' in doc.name:
            loader = UnstructuredPowerPointLoader(file_name)
            documents = loader.load_and_split()

        doc_list.extend(documents)
    return doc_list

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100,
        length_function=tiktoken_len
    )
    chunks = text_splitter.split_documents(text)
    return chunks

class SentenceTransformerEmbeddings:
    """SentenceTransformerë¥¼ LangChainì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë˜í¼"""
    def __init__(self, model_path):
        self.model = SentenceTransformer(model_path)

    def embed_documents(self, texts):
        return self.model.encode(texts).tolist()

    def embed_query(self, text):
        return self.model.encode([text])[0].tolist()

def get_vectorstore(text_chunks, use_finetuned=False):
    # íŒŒì¸íŠœë‹ëœ ì„ë² ë”© ëª¨ë¸ ë˜ëŠ” ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
    if use_finetuned:
        embeddings = SentenceTransformerEmbeddings(EMBEDDING_PATH)
    else:
        embeddings = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    vectordb = FAISS.from_documents(text_chunks, embeddings)

    return vectordb

@st.cache_resource
def load_peft_model():
    """íŒŒì¸íŠœë‹ëœ LoRA ëª¨ë¸ ë¡œë“œ (ìºì‹±)"""
    st.info("íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(LLM_LORA_PATH)

    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_LLM_MODEL,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    model = PeftModel.from_pretrained(base_model, LLM_LORA_PATH)

    # Pipeline ìƒì„±
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1
    )

    # LangChain LLMìœ¼ë¡œ ë˜í•‘
    llm = HuggingFacePipeline(pipeline=pipe)

    st.success("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    return llm

def extract_text_from_chunk(chunk):
    """ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if isinstance(chunk, str):
        return chunk
    elif isinstance(chunk, dict):
        content = chunk.get('content', '') or chunk.get('text', '')
        return content if content else ''
    elif hasattr(chunk, 'content'):
        return chunk.content if chunk.content else ''
    else:
        return ''

def main():

    global retriever

    st.set_page_config(
    page_title="ì±—ë´‡",
    page_icon="")

    st.title(":blue[ì±—ë´‡]")
    st.caption("ğŸš€ RAG + PEFT í•˜ì´ë¸Œë¦¬ë“œ ë²„ì „ (CSV ì§ì› ë°ì´í„° í¬í•¨)")

    if "messages" not in st.session_state:
       st.session_state["messages"] = []

   #ì±„íŒ… ëŒ€í™”ê¸°ë¡ì„ ì ê²€
    if "store" not in st.session_state:
       st.session_state["store"] =dict()


    def print_history():
        for msg in st.session_state.messages:
            st.chat_message(msg.role).write(msg.content)

    def add_history(role, content):
        st.session_state.messages.append(ChatMessage(role=role, content=content))

    if "processComplete" not in st.session_state:
        st.session_state.processComplete = None

    if "retriever" not in st.session_state:
        st.session_state.retriever = None

    with st.sidebar:
        st.header("ìë£Œ ì—…ë¡œë“œ")

        # CSV ìë™ ë¡œë“œ ì˜µì…˜
        load_csv = st.checkbox("íšŒì‚¬ ì§ì› ë°ì´í„° ìë™ ë¡œë“œ", value=True)

        uploaded_files =  st.file_uploader("PDF, DOCX, PPTX íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
                                          type=['pdf','docx','pptx'],
                                          accept_multiple_files=True)

        # ëª¨ë¸ ì„ íƒ
        use_peft = st.checkbox("PEFT íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©", value=False,
                               help="ì²´í¬ ì‹œ ë¡œì»¬ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš© (ëŠë¦¼), ë¯¸ì²´í¬ ì‹œ RemoteRunnable ì‚¬ìš© (ë¹ ë¦„)")

        process = st.button("ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°")

    if process:
        all_docs = []

        # ì—…ë¡œë“œ íŒŒì¼ ì²˜ë¦¬
        if uploaded_files:
            files_text = get_text(uploaded_files)
            text_chunks = get_text_chunks(files_text)
            all_docs.extend(text_chunks)

        # CSV ë°ì´í„° ì¶”ê°€
        if load_csv:
            st.info("íšŒì‚¬ ì§ì› ë°ì´í„° ë¡œë“œ ì¤‘...")
            csv_docs = load_csv_data(CSV_DATA_PATH)
            all_docs.extend(csv_docs)
            st.success(f"{len(csv_docs)}ëª…ì˜ ì§ì› ì •ë³´ ë¡œë“œ ì™„ë£Œ!")

        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        if all_docs:
            vectorstore = get_vectorstore(all_docs, use_finetuned=False)
            retriever = vectorstore.as_retriever(
                search_type='mmr',
                search_kwargs={'k': 5, 'fetch_k': 10}
            )
            st.session_state['retriever'] = retriever
            st.session_state['use_peft'] = use_peft
            st.session_state.processComplete = True
            st.success(f"ì´ {len(all_docs)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")
        else:
            st.error("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [{"role": "assistant",
                                        "content": "ì•ˆë…•í•˜ì„¸ìš”! Future Systems íšŒì‚¬ì†Œê°œ ì±—ë´‡ì…ë‹ˆë‹¤. íšŒì‚¬ ë° ì§ì› ì •ë³´ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"}]

    def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
        return "\n\n".join(doc.page_content for doc in docs)

    RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ Future Systems íšŒì‚¬ ì†Œê°œ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                             ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ íšŒì‚¬ ë° ì§ì› ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                             íŠ¹íˆ ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ë¶€ì„œ ë“±ì˜ ì •ë³´ëŠ” ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì œê³µí•˜ì„¸ìš”.

                             Question: {question}
                             Context: {context}
                             Answer:"""

    print_history()

    if user_input := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”..."):
        #ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
        add_history("user", user_input)
        st.chat_message("user").write(f"{user_input}")
        with st.chat_message("assistant"):

            chat_container = st.empty()

            if  st.session_state.processComplete==True:
                # LLM ì„ íƒ (PEFT ë˜ëŠ” RemoteRunnable)
                if st.session_state.get('use_peft', False):
                    from langserve import RemoteRunnable
                    llm = RemoteRunnable("https://dioramic-corrin-undetractively.ngrok-free.dev/llm/")
                    # llm = load_peft_model()  # PEFT ëª¨ë¸ (GPU í•„ìš”)
                else:
                    from langserve import RemoteRunnable
                    llm = RemoteRunnable("https://dioramic-corrin-undetractively.ngrok-free.dev/llm/")

                prompt1 = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

                retriever = st.session_state['retriever']
               # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
                rag_chain = (
                   {
                       "context": retriever | format_docs,
                       "question": RunnablePassthrough(),
                   }
                   | prompt1
                   | llm
                )

                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                answer = rag_chain.stream(user_input)
                chunks = []
                for chunk in answer:
                    chunk_text = extract_text_from_chunk(chunk)
                    if chunk_text:
                        chunks.append(chunk_text)
                        chat_container.markdown("".join(chunks))
                add_history("ai", "".join(chunks))

            else:
                st.warning("ë¨¼ì € 'ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")

if __name__ == '__main__':
    main()
